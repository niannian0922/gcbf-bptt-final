# Final Champion V8 Phoenix Configuration - Fully Differentiable Guardian
# Phoenix Plan: Safety by Architecture, Loss for Efficiency

# Top-level device selection
device: "cuda"

# Evaluation-specific configuration
evaluation:
  run_name: "final_champion_v8_phoenix"
  model_step: 10000
  num_episodes: 1000
  device: "cuda"

# Optional run name at top-level (used by trainer fallback)
run_name: "final_champion_v8_phoenix"

# Model and loss type for Phoenix Plan
model_type: "phoenix"
loss_type: "phoenix"

# Environment configuration (must satisfy SingleAgentEnv and SingleDoubleIntegratorEnv)
env:
  env_type: "single_double_integrator"
  area_size: 2.0
  dt: 0.05
  max_steps: 256
  agent_radius: 0.05
  mass: 0.1
  max_force: 1.0
  cbf_alpha: 1.0
  vision:
    enabled: false
    image_size: 64
    camera_fov: 90.0
    camera_range: 3.0
  obstacles:
    enabled: true
    positions: [[1.0, 1.0]]
    radii: [0.2]
    dynamic_count: false
    random: false
    random_count: 0
    random_min_radius: 0.08
    random_max_radius: 0.3
    count_range: [2, 8]

# Policy configuration (BPTTPolicy for Phoenix)
policy:
  hidden_dim: 128
  type: "bptt"
  perception:
    vision_enabled: false
    input_dim: 9
    hidden_dim: 128
    num_layers: 2
    activation: "relu"
    use_batch_norm: false
    input_channels: 1
    conv_channels: [32, 64, 128]
    kernel_sizes: [5, 3, 3]
    image_size: 64
  memory:
    input_dim: 128
    hidden_dim: 128
    num_layers: 1
    dropout: 0.0
  policy_head:
    input_dim: 128
    output_dim: 2
    hidden_dims: [256, 256]
    activation: "relu"
    output_activation: null
    action_scale: 1.0
    predict_alpha: true
    alpha_hidden_dim: 64
    predict_margin: false
    margin_hidden_dim: 32
  actor:
    hidden_dims: [128, 128]
    activation: "relu"
    output_dim: 2
    output_activation: null
    learning_rate: 0.001
  critic:
    hidden_dims: [128, 128]
    activation: "relu"
    learning_rate: 0.001

# Trainer configuration
trainer:
  run_name: "final_champion_v8_phoenix"
  log_dir: "logs"
  bptt:
    horizon_length: 64
  optim:
    lr: 0.001

# Unified training schema
training:
  training_steps: 10000
  horizon_length: 64
  learning_rate: 0.001
  save_interval: 1000
  max_grad_norm: 10.0
  gradient_decay_rate: 0.95

# Weights & Biases configuration
wandb:
  project: "gcbf-bptt-final"
  entity: "jihaoye0922"
  mode: "offline"

# Phoenix Loss function weights
losses:
  eff_weight: 1.0
  correction_weight: 0.1
  jerk_weight: 0.05

# GCBF safety parameters
gcbf:
  enabled: true
  alpha: 1.0
  safety_margin: 0.1
  max_iterations: 10

# Model architecture parameters
model:
  use_ensemble: false
  ensemble_method: "mean"
  num_policies: 3

# Logging and monitoring
logging:
  log_interval: 10
  eval_interval: 100
  tensorboard: true
  save_trajectories: false
